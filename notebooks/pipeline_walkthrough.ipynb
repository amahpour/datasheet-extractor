{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e36fa899",
      "metadata": {},
      "source": [
        "# Datasheet Extractor Walkthrough (Notebook)\n",
        "\n",
        "This notebook is written as a hands-on tutorial for engineers building **IC device drivers**.\n",
        "\n",
        "Instead of jumping straight to the final CLI output, we walk the same path your app takes:\n",
        "\n",
        "1. Start with plain Docling conversion (baseline behavior)\n",
        "2. Add project-specific processing (chunking, figure artifacts, local LLM triage)\n",
        "3. Produce organized, stable outputs for downstream driver workflows\n",
        "\n",
        "By the end, you should clearly see what each layer contributes and why the final pipeline is more useful than raw PDF parsing alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b5ed4d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-16T23:56:54.135854Z",
          "iopub.status.busy": "2026-02-16T23:56:54.135640Z",
          "iopub.status.idle": "2026-02-16T23:56:54.143216Z",
          "shell.execute_reply": "2026-02-16T23:56:54.141753Z"
        }
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "# Resolve repo root whether notebook is run from repo root or notebooks/.\n",
        "cwd = Path.cwd().resolve()\n",
        "REPO_ROOT = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
        "\n",
        "# Optional local-only env overrides (not committed): notebooks/.local_env.json\n",
        "# Example:\n",
        "# {\n",
        "#   \"OLLAMA_HOST\": \"http://172.26.0.1:11434\",\n",
        "#   \"ANTHROPIC_BASE_URL\": \"http://172.26.0.1:11434\",\n",
        "#   \"ANTHROPIC_AUTH_TOKEN\": \"ollama\",\n",
        "#   \"ANTHROPIC_API_KEY\": \"\"\n",
        "# }\n",
        "local_env_path = REPO_ROOT / \"notebooks\" / \".local_env.json\"\n",
        "if local_env_path.exists():\n",
        "    local_env = json.loads(local_env_path.read_text(encoding=\"utf-8\"))\n",
        "    for key, value in local_env.items():\n",
        "        os.environ.setdefault(key, str(value))\n",
        "\n",
        "# Safe defaults for local usage; override via shell env or .local_env.json.\n",
        "os.environ.setdefault(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\")\n",
        "print(f\"OLLAMA_HOST: {os.environ.get('OLLAMA_HOST')}\")\n",
        "\n",
        "PDF_PATH = REPO_ROOT / \"examples\" / \"dac7578\" / \"dac5578.pdf\"\n",
        "WORK_ROOT = REPO_ROOT / \"out_notebook\"\n",
        "\n",
        "# Config knobs for runtime: default to one page for fast demos.\n",
        "PAGES = \"1\"\n",
        "MAX_TOKENS = 256\n",
        "\n",
        "if not PDF_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing sample PDF: {PDF_PATH}\")\n",
        "\n",
        "print(f\"Repo root: {REPO_ROOT}\")\n",
        "print(f\"PDF:       {PDF_PATH}\")\n",
        "print(f\"Work dir:  {WORK_ROOT}\")\n",
        "print(f\"PAGES:     {PAGES}\")\n",
        "\n",
        "# Local LLM demo controls.\n",
        "STRICT_LOCAL_LLM = True  # Enforce local-model-only execution for this walkthrough.\n",
        "\n",
        "from src.local_processor import _detect_ollama_model\n",
        "LOCAL_VISION_MODEL = _detect_ollama_model()\n",
        "print(f\"Local vision model detected: {LOCAL_VISION_MODEL}\")\n",
        "if not LOCAL_VISION_MODEL:\n",
        "    raise RuntimeError(\"No Ollama vision model detected. Start Ollama, run `ollama pull moondream`, and re-run the notebook.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29bbaf3c",
      "metadata": {},
      "source": [
        "## Stage 1: Docling Out of the Box\n",
        "\n",
        "We begin with **default Docling** behavior and no project-specific logic.\n",
        "\n",
        "### Why this stage matters\n",
        "- It establishes the baseline quality and structure you get from a strong general-purpose parser.\n",
        "- It shows the raw signal available before schema normalization and figure triage.\n",
        "\n",
        "### What to look for\n",
        "- Total pages, tables, and image-like items (`PictureItem`)\n",
        "- A markdown preview of raw document export\n",
        "\n",
        "This tells us what Docling can do alone, and what gaps we still need to fill for driver-oriented extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d08a72bf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-16T23:56:54.145144Z",
          "iopub.status.busy": "2026-02-16T23:56:54.144988Z",
          "iopub.status.idle": "2026-02-16T23:58:05.687530Z",
          "shell.execute_reply": "2026-02-16T23:58:05.686368Z"
        }
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocling_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PictureItem, TableItem\n\u001b[32m      4\u001b[39m converter = DocumentConverter()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m conv_res = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPDF_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m doc = conv_res.document\n\u001b[32m      8\u001b[39m num_pages = \u001b[38;5;28mgetattr\u001b[39m(doc, \u001b[33m\"\u001b[39m\u001b[33mnum_pages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:39\u001b[39m, in \u001b[36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(wrapped)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_function\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/pydantic/_internal/_validate_call.py:136\u001b[39m, in \u001b[36mValidateCallWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__pydantic_complete__:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m._create_validators()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__return_pydantic_validator__:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__return_pydantic_validator__(res)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/document_converter.py:336\u001b[39m, in \u001b[36mDocumentConverter.convert\u001b[39m\u001b[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert one document fetched from a file path, URL, or DocumentStream.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03mNote: If the document content is given as a string (Markdown or HTML\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m \u001b[33;03m    ConversionError: An error occurred during conversion.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    328\u001b[39m all_res = \u001b[38;5;28mself\u001b[39m.convert_all(\n\u001b[32m    329\u001b[39m     source=[source],\n\u001b[32m    330\u001b[39m     raises_on_error=raises_on_error,\n\u001b[32m   (...)\u001b[39m\u001b[32m    334\u001b[39m     page_range=page_range,\n\u001b[32m    335\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_res\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/document_converter.py:379\u001b[39m, in \u001b[36mDocumentConverter.convert_all\u001b[39m\u001b[34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[39m\n\u001b[32m    376\u001b[39m conv_res_iter = \u001b[38;5;28mself\u001b[39m._convert(conv_input, raises_on_error=raises_on_error)\n\u001b[32m    378\u001b[39m had_result = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhad_result\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPARTIAL_SUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/document_converter.py:478\u001b[39m, in \u001b[36mDocumentConverter._convert\u001b[39m\u001b[34m(self, conv_input, raises_on_error)\u001b[39m\n\u001b[32m    476\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m item\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocess_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/document_converter.py:525\u001b[39m, in \u001b[36mDocumentConverter._process_document\u001b[39m\u001b[34m(self, in_doc, raises_on_error)\u001b[39m\n\u001b[32m    521\u001b[39m valid = (\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mself\u001b[39m.allowed_formats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m in_doc.format \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.allowed_formats\n\u001b[32m    523\u001b[39m )\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     conv_res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m     error_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile format not allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_doc.file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/document_converter.py:548\u001b[39m, in \u001b[36mDocumentConverter._execute_pipeline\u001b[39m\u001b[34m(self, in_doc, raises_on_error)\u001b[39m\n\u001b[32m    546\u001b[39m pipeline = \u001b[38;5;28mself\u001b[39m._get_pipeline(in_doc.format)\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     conv_res = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raises_on_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/pipeline/base_pipeline.py:75\u001b[39m, in \u001b[36mBasePipeline.execute\u001b[39m\u001b[34m(self, in_doc, raises_on_error)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m TimeRecorder(\n\u001b[32m     71\u001b[39m         conv_res, \u001b[33m\"\u001b[39m\u001b[33mpipeline_total\u001b[39m\u001b[33m\"\u001b[39m, scope=ProfilingScope.DOCUMENT\n\u001b[32m     72\u001b[39m     ):\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# These steps are building and assembling the structure of the\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# output DoclingDocument.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         conv_res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m         conv_res = \u001b[38;5;28mself\u001b[39m._assemble_document(conv_res)\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# From this stage, all operations should rely only on conv_res.output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/pipeline/standard_pdf_pipeline.py:674\u001b[39m, in \u001b[36mStandardPdfPipeline._build_document\u001b[39m\u001b[34m(self, conv_res)\u001b[39m\n\u001b[32m    671\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[38;5;66;03m# 2) drain - pull whatever is ready from the output side\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m out_batch = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m itm \u001b[38;5;129;01min\u001b[39;00m out_batch:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m itm.run_id != run_id:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/datasheet-extractor-oK-IPdPc-py3.12/lib/python3.12/site-packages/docling/pipeline/standard_pdf_pipeline.py:162\u001b[39m, in \u001b[36mThreadedQueue.get_batch\u001b[39m\u001b[34m(self, size, timeout)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0\u001b[39m:\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_not_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m._not_empty.wait()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "from docling_core.types.doc import PictureItem, TableItem\n",
        "\n",
        "converter = DocumentConverter()\n",
        "conv_res = converter.convert(str(PDF_PATH))\n",
        "doc = conv_res.document\n",
        "\n",
        "num_pages = getattr(doc, \"num_pages\", None)\n",
        "num_pages = num_pages() if callable(num_pages) else num_pages\n",
        "if num_pages is None:\n",
        "    num_pages = len(getattr(doc, \"pages\", {}))\n",
        "\n",
        "picture_count = 0\n",
        "table_item_count = 0\n",
        "for item, _ in doc.iterate_items():\n",
        "    if isinstance(item, PictureItem):\n",
        "        picture_count += 1\n",
        "    elif isinstance(item, TableItem):\n",
        "        table_item_count += 1\n",
        "\n",
        "print(\"Docling baseline summary\")\n",
        "print(\"- num_pages:\", num_pages)\n",
        "print(\"- doc.tables:\", len(getattr(doc, \"tables\", [])))\n",
        "print(\"- PictureItem count:\", picture_count)\n",
        "print(\"- TableItem count:\", table_item_count)\n",
        "\n",
        "if hasattr(doc, \"export_to_markdown\"):\n",
        "    md = doc.export_to_markdown()\n",
        "    print(\"\\nDocling markdown preview (first 1000 chars):\\n\")\n",
        "    print(md[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0ea555",
      "metadata": {},
      "source": [
        "### Interpreting Stage 1\n",
        "\n",
        "If Docling finds many `PictureItem`s, that usually means it is extracting every image-like object in the PDF, not just semantic diagrams.\n",
        "\n",
        "That is useful raw coverage, but it also explains why downstream filtering/organization is needed before these assets are practical for driver-focused use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31d4ecc",
      "metadata": {},
      "source": [
        "## Stage 2: Project Enhancements on Top of Docling\n",
        "\n",
        "Now we run the same core extraction logic used in this repository.\n",
        "\n",
        "### Enhancements introduced here\n",
        "- **Hybrid chunking** with heading context and `enriched_text`\n",
        "- **Figure export** into stable IDs (`fig_0001`, etc.)\n",
        "- Optional **local LLM triage** to classify figures and route complex ones\n",
        "\n",
        "### Why this matters for driver development\n",
        "These steps make it easier to isolate register-relevant text/tables and identify figures that need higher-quality interpretation before they can be trusted in implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f783e02",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-16T23:58:05.689739Z",
          "iopub.status.busy": "2026-02-16T23:58:05.689562Z",
          "iopub.status.idle": "2026-02-16T23:59:14.203071Z",
          "shell.execute_reply": "2026-02-16T23:59:14.201766Z"
        }
      },
      "outputs": [],
      "source": [
        "from src.extract_docling import extract_document, to_blocks\n",
        "\n",
        "stage2_dir = WORK_ROOT / \"stage2_docling_plus\"\n",
        "shutil.rmtree(stage2_dir, ignore_errors=True)\n",
        "stage2_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "raw = extract_document(PDF_PATH, out_dir=stage2_dir, max_tokens=MAX_TOKENS)\n",
        "blocks = to_blocks(raw.get(\"blocks\", []))\n",
        "\n",
        "if PAGES:\n",
        "    keep = set()\n",
        "    for token in PAGES.split(\",\"):\n",
        "        token = token.strip()\n",
        "        if \"-\" in token:\n",
        "            a, b = token.split(\"-\", 1)\n",
        "            for page in range(int(a), int(b) + 1):\n",
        "                keep.add(page)\n",
        "        elif token:\n",
        "            keep.add(int(token))\n",
        "    blocks = [b for b in blocks if b.page in keep]\n",
        "    raw[\"tables\"] = [t for t in raw.get(\"tables\", []) if int(t.get(\"page\", 1)) in keep]\n",
        "    raw[\"figures\"] = [f for f in raw.get(\"figures\", []) if int(f.get(\"page\", 1)) in keep]\n",
        "\n",
        "print(\"Project extraction summary\")\n",
        "print(\"- page_count:\", raw.get(\"page_count\"))\n",
        "print(\"- blocks:\", len(blocks))\n",
        "print(\"- tables:\", len(raw.get(\"tables\", [])))\n",
        "print(\"- figures metadata:\", len(raw.get(\"figures\", [])))\n",
        "print(\"- figure images on disk:\", len(list((stage2_dir / \"figures\").glob(\"fig_*.png\"))))\n",
        "\n",
        "if blocks:\n",
        "    sample = blocks[0]\n",
        "    print(\"\\nSample block:\")\n",
        "    print(\"- id:\", sample.id)\n",
        "    print(\"- page:\", sample.page)\n",
        "    print(\"- headings:\", sample.headings)\n",
        "    print(\"- text preview:\", sample.text[:250])\n",
        "    print(\"- enriched preview:\", sample.enriched_text[:250])\n",
        "\n",
        "if raw.get(\"tables\"):\n",
        "    print(\"\\nSample table (header row):\")\n",
        "    print(raw[\"tables\"][0].get(\"grid\", [[]])[0])\n",
        "\n",
        "if raw.get(\"figures\"):\n",
        "    print(\"\\nSample figure metadata:\")\n",
        "    pprint(raw[\"figures\"][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e3a474",
      "metadata": {},
      "source": [
        "### Local LLM Triage (Optional but Useful)\n",
        "\n",
        "This next cell runs local figure processing to classify and summarize extracted images.\n",
        "\n",
        "Use it as a **routing signal**, not ground truth: simple artifacts can resolve locally, while complex technical figures should be escalated to a stronger external model or manual review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5b22e6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-16T23:59:14.205909Z",
          "iopub.status.busy": "2026-02-16T23:59:14.205544Z",
          "iopub.status.idle": "2026-02-17T00:00:21.529007Z",
          "shell.execute_reply": "2026-02-17T00:00:21.527521Z"
        }
      },
      "outputs": [],
      "source": [
        "from src.local_processor import build_rollup, process_all_figures\n",
        "\n",
        "figures_dir = stage2_dir / \"figures\"\n",
        "processing_dir = stage2_dir / \"processing\"\n",
        "processing_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fig_paths = sorted(figures_dir.glob(\"fig_*.png\"))\n",
        "demo_n = min(12, len(fig_paths))\n",
        "demo_ids = {p.stem for p in fig_paths[:demo_n]}\n",
        "\n",
        "print(f\"Running local figure processing on {demo_n} figure(s)...\")\n",
        "statuses = process_all_figures(\n",
        "    figures_dir=figures_dir,\n",
        "    processing_dir=processing_dir,\n",
        "    ollama_model=LOCAL_VISION_MODEL,\n",
        "    force=True,\n",
        "    figure_ids=demo_ids,\n",
        ")\n",
        "\n",
        "rollup = build_rollup(statuses)\n",
        "print(\"\\nLocal processing rollup (subset):\")\n",
        "pprint(rollup[\"summary\"])\n",
        "\n",
        "ran_local_llm = sum(1 for s in statuses if s.get(\"stage\") == \"local_llm\")\n",
        "print(f\"Local LLM-processed figures in subset: {ran_local_llm}/{len(statuses)}\")\n",
        "\n",
        "print(\"\\nFirst 3 per-figure statuses:\")\n",
        "for status in statuses[:3]:\n",
        "    pprint({\n",
        "        \"figure_id\": status[\"figure_id\"],\n",
        "        \"stage\": status.get(\"stage\", \"\"),\n",
        "        \"status\": status[\"status\"],\n",
        "        \"classification\": status.get(\"local_llm_classification\", \"\"),\n",
        "        \"needs_external\": status[\"needs_external\"],\n",
        "        \"confidence\": status[\"confidence\"],\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86646eec",
      "metadata": {},
      "source": [
        "## Stage 3: Full App Pipeline and Organized Outputs\n",
        "\n",
        "In this stage we run `process_pdf(...)`, the same end-to-end flow used by the CLI.\n",
        "\n",
        "### What this adds beyond Stage 2\n",
        "- Canonical `document.json`\n",
        "- Per-format table exports (`json/csv/md`)\n",
        "- Per-figure processing status files\n",
        "- Manual-followup reports and rollups\n",
        "\n",
        "This is the transition from a technical demo to a reproducible workflow you can hand to teammates or downstream automation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd0bfd1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T00:00:21.531459Z",
          "iopub.status.busy": "2026-02-17T00:00:21.531193Z",
          "iopub.status.idle": "2026-02-17T00:01:52.818044Z",
          "shell.execute_reply": "2026-02-17T00:01:52.816295Z"
        }
      },
      "outputs": [],
      "source": [
        "from src.pipeline import process_pdf\n",
        "\n",
        "stage3_root = WORK_ROOT / \"stage3_app\"\n",
        "shutil.rmtree(stage3_root, ignore_errors=True)\n",
        "stage3_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result = process_pdf(\n",
        "    pdf_path=PDF_PATH,\n",
        "    out_root=stage3_root,\n",
        "    pages=PAGES,\n",
        "    force=True,\n",
        "    no_images=False,\n",
        "    no_tables=False,\n",
        "    max_figures=None,\n",
        "    ollama_model=LOCAL_VISION_MODEL,\n",
        "    max_tokens=MAX_TOKENS,\n",
        ")\n",
        "\n",
        "pdf_out = Path(result[\"out_dir\"])\n",
        "doc_json = pdf_out / \"document.json\"\n",
        "index_json = pdf_out / \"index.json\"\n",
        "manual_json = pdf_out / \"manual_processing_report.json\"\n",
        "rollup_json = pdf_out / \"processing_rollup.json\"\n",
        "\n",
        "print(\"App output directory:\", pdf_out)\n",
        "print(\"document.json exists:\", doc_json.exists())\n",
        "print(\"index.json exists:\", index_json.exists())\n",
        "print(\"manual report exists:\", manual_json.exists())\n",
        "print(\"processing rollup exists:\", rollup_json.exists())\n",
        "\n",
        "doc = json.loads(doc_json.read_text(encoding=\"utf-8\"))\n",
        "print(\"\\ndoc_stats:\")\n",
        "pprint(doc[\"doc_stats\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8f7ea0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T00:01:52.821028Z",
          "iopub.status.busy": "2026-02-17T00:01:52.820807Z",
          "iopub.status.idle": "2026-02-17T00:01:52.829775Z",
          "shell.execute_reply": "2026-02-17T00:01:52.828039Z"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Key files under out_notebook/stage3_app/<pdf_stem>/\")\n",
        "for rel in [\n",
        "    \"document.json\",\n",
        "    \"index.json\",\n",
        "    \"manual_processing_report.json\",\n",
        "    \"manual_processing_report.md\",\n",
        "    \"processing_rollup.json\",\n",
        "    \"processing_rollup.md\",\n",
        "    \"tables\",\n",
        "    \"figures\",\n",
        "    \"processing\",\n",
        "    \"derived\",\n",
        "]:\n",
        "    p = pdf_out / rel\n",
        "    marker = \"dir\" if p.is_dir() else \"file\"\n",
        "    print(f\"- [{marker}] {p.relative_to(stage3_root)}\")\n",
        "\n",
        "table_files = sorted((pdf_out / \"tables\").glob(\"*\"))\n",
        "figure_files = sorted((pdf_out / \"figures\").glob(\"fig_*.png\"))\n",
        "status_files = sorted((pdf_out / \"processing\").glob(\"fig_*.json\"))\n",
        "\n",
        "print(\"\\nCounts\")\n",
        "print(\"- tables artifacts:\", len(table_files))\n",
        "print(\"- figure images:\", len(figure_files))\n",
        "print(\"- processing status files:\", len(status_files))\n",
        "\n",
        "if rollup_json.exists():\n",
        "    rollup = json.loads(rollup_json.read_text(encoding=\"utf-8\"))\n",
        "    print(\"\\nprocessing_rollup.summary:\")\n",
        "    pprint(rollup[\"summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b12ce9",
      "metadata": {},
      "source": [
        "### What You Should Have at This Point\n",
        "\n",
        "You now have a complete per-document package with normalized content and explicit processing state.\n",
        "\n",
        "This notebook is local-model-only: figure triage runs with your local Ollama vision model (for example `moondream`).\n",
        "If no local model is available, the notebook fails fast so results are never mixed with fallback behavior.\n",
        "\n",
        "That package is the bridge from a static datasheet PDF to driver-development tasks like register abstraction, validation checklists, and implementation traceability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
